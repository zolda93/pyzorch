from collections import defaultdict,abc
from zorch import*
from itertools import chain


#adapted from https://pytorch.org/docs/stable/_modules/torch/optim/optimizer.html#Optimizer

class _RequiredParameter:
    def __repr__(self):
        return "<required parameter>"


required = _RequiredParameter()

class Optimizer:
    def __init__(self,params,defaults):
        self.defaults = defaults

        if isinstance(params,Tensor):
            raise ValueError("params argument given to the optimizer should be "
                            "an iterable of Tensors or dicts, but got " +
                            params.__class__.__name__)

        self.state = defaultdict(dict)
        self.param_groups = []

        param_groups = list(params)
        if len(param_groups) == 0:
            raise ValueError("optimizer got an empty parameter list")
        if not isinstance(param_groups[0],dict):
            param_groups = [{'params':param_groups}]
        for param_group in param_groups:
            self.add_param_group(param_group)

    def __getstate__(self):
        return {'defaults':self.defaults,'state':self.state,'param_groups':self.param_groups}
    def __setstate__(self,state):
        self.__dict__.update(state)

    def __repr__(self):
        format_string = self.__class__.__name__ + ' ('
        for i, group in enumerate(self.param_groups):
            format_string += '\n'
            format_string += 'Parameter Group {0}\n'.format(i)
            for key in sorted(group.keys()):
                if key != 'params':
                    format_string += '    {0}: {1}\n'.format(key, group[key])
        format_string += ')'
        return format_string

    def state_dict(self):
        param_mappings = {}
        start_index = 0

        def pack_group(group):
            nonlocal start_index
            packed = {k: v for k, v in group.items() if k != 'params'}
            param_mappings.update({id(p): i for i, p in enumerate(group['params'], start_index)
                                   if id(p) not in param_mappings})
            packed['params'] = [param_mappings[id(p)] for p in group['params']]
            start_index += len(packed['params'])
            return packed
        param_groups = [pack_group(g) for g in self.param_groups]
        # Remap state to use order indices as keys
        # convert arrays to numpy array (always saves using numpy array, to be consistent to nn.Module)
        def convert_to_numpy(adict): # i.e. {'momentum_buffer': some cupy array}
            return {k:v.get() if hasattr(v,'device') else v for k,v in adict.items()}

        packed_state = {(param_mappings[id(k)] if isinstance(k, Tensor) else k): convert_to_numpy(v)
                        for k, v in self.state.items()}
        return {'state': packed_state,'param_groups': param_groups,}

    def load_state_dict(self,stat_dict):
        state_dict = state_dict.copy()
        # Validate the state_dict
        groups = self.param_groups
        saved_groups = state_dict['param_groups']

        if len(groups) != len(saved_groups):
            raise ValueError("loaded state dict has a different number of "
                             "parameter groups")
        param_lens = (len(g['params']) for g in groups)
        saved_lens = (len(g['params']) for g in saved_groups)
        if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):
            raise ValueError("loaded state dict contains a parameter group "
                             "that doesn't match the size of optimizer's group")

        # Update the state
        id_map = {old_id: p for old_id, p in
                  zip(chain.from_iterable((g['params'] for g in saved_groups)),
                      chain.from_iterable((g['params'] for g in groups)))}

        def cast(param, value):
            r"""Make a deep copy of value, casting all tensors to device of param."""
            # saved `value` is always numpy arrays. if `value` is numpy array but `param` is cupy array,
            # convert `value` to cupy array.
            if isinstance(value, np.ndarray):
                if isinstance(param, cp.ndarray):
                    return cp.array(value)
                else:
                    return value
            elif isinstance(value, dict):
                return {k: cast(param, v) for k, v in value.items()}
            elif isinstance(value, abc.Iterable):
                return type(value)(cast(param, v) for v in value)
            else:
                return value

        # Copy state assigned to params (and cast tensors to appropriate types).
        # State that is not assigned to params is copied as is (needed for
        # backward compatibility).
        state = defaultdict(dict)
        for k, v in state_dict['state'].items():
            if k in id_map:
                param = id_map[k]
                state[param] = cast(param, v)
            else:
                state[k] = v

        # Update parameter groups, setting their 'params' value
        def update_group(group, new_group):
            new_group['params'] = group['params']
            return new_group
        param_groups = [
            update_group(g, ng) for g, ng in zip(groups, saved_groups)]
        self.__setstate__({'state': state, 'param_groups': param_groups})

    def zero_grad(self):
        for group in self.param_groups:
            for p in group['params']:
                p.zero_grad()

    def step(self):
        raise NotImplementedError

    def add_param_group(self,param_group):
        assert isinstance(param_group, dict), "param group must be a dict"

        params = param_group['params']
        if isinstance(params, Tensor):
            param_group['params'] = [params]
        elif isinstance(params, set):
            raise TypeError('optimizer parameters need to be organized in ordered collections, but '
                            'the ordering of tensors in sets will change between runs. Please use a list instead.')
        else:
            param_group['params'] = list(params)

        for param in param_group['params']:
            if not isinstance(param, Tensor):
                raise TypeError("optimizer can only optimize Tensors, "
                                "but one of the params is " + param.__class__.__name__)
            if param.op is not None:
                if not len(param.op.parents) == 0:
                    raise ValueError("can't optimize a non-leaf Tensor")

        for name, default in self.defaults.items():
            if default is required and name not in param_group:
                raise ValueError("parameter group didn't specify a value of required optimization parameter " +
                                 name)
            else:
                param_group.setdefault(name, default)

        params = param_group['params']
        if len(params) != len(set(params)):
            import warnings
            warnings.warn("optimizer contains a parameter group with duplicate parameters.", stacklevel=3)

        param_set = set()
        for group in self.param_groups:
            param_set.update(set(group['params']))

        if not param_set.isdisjoint(set(param_group['params'])):
            raise ValueError("some parameters appear in more than one parameter group")

        self.param_groups.append(param_group)


